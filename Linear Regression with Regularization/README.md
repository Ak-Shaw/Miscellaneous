# Linear Regression with Regularization

## Introduction

**Linear Regression** is the most simple, yet the most important statistical technique to estimate the relationship between the independant and the dependant features. It aims at predicting a **continuous variable** like house prices, sales of a company etc.<br>

In this micro task, we will take a look at linear regression by solving a problem of **predicting the price of a house given its attributes or features.** Along with this, we will explore types of regularization techniques that can be used with Linear Regression. Regularization means to make the model insensitive to anomalies (or often referred to as outliers) present in the data. This helps in making the model more generalized to the unseen data.

## Prerequisites

For this micro-task, you should have a basic knowledge of:
* Python
* Pandas
* Matplotlib

These libraries form the basis of any Machine Learnig Project or Algorithm.

Along with this, we will be using **Jupyter Notebooks** as our working environment. So make sure you have it configured for your pc or laptop. You can have a look at the installation instructions [here](https://test-jupyter.readthedocs.io/en/latest/install.html).

Now, you are ready for this exciting micro-byte!

## Activities

**Note:** Remember to go through the additional material provided in the activities to have a more thorough understanding of the underlying concepts.

### Activity 1 - Understanding Linear Regression

Lets dive into understanding and implementing a basic Linear Regression model on the given dataset. 

> *Refer to src/Activity 1/Activity 1 - Understanding Linear Regression.ipynb*

### Activity 2 - Assumptions of Linear Regression

Now, hopefully you have understood the algorithm of Linear Regression and can apply it to any given dataset. But, it is also worth noticing that in order to apply Linear Regression, there are some conditions that are to be fulfilled or there are some assumptions which Linear Regression assumes for the data on which it is applied.

To get the best out of the algorithm, we must check if these assumptions are satisfied. Hence, let us learn about it in this activity.

> *Refer to src/Activity 2/Activity 2 - Assumptions of Linear Regression.ipynb*

### Activity 3 - Ridge, Lasso and ElasticNet Regression

Congratulations!!! You now have complete knowledge of Linear Regression and are ready to get into the more complex algorithms.

By this time, you would have understood that Linear Regression is sensitive to outliers. This is because the cost function on which it is evaluated, increases unusually in the presence of outliers. This sensitivity of the algorithm results in a completely different model than a usual one.

To overcome this behaviour, we have 3 different kinds of regularization techniques. These are **Ridge, Lasso, and ElasticNet** Regression. Let us learn about them in detail in this activity.

> *Refer to src/Activity 3/Activity 3 - Ridge, Lasso and ElasticNet Regression.ipynb*

### Activity 4 - Detailed Data Analysis

Congratulations!!! You now have complete knowledge of Linear Regression and are ready to get into the more complex algorithms.

By this time, you would have understood that Linear Regression is sensitive to outliers. This is because the cost function on which it is evaluated, increases unusually in the presence of outliers. This sensitivity of the algorithm results in a completely different model than a usual one.

To understand our data and detecting the outliers we use a concept. These are **Detailed Data Analysis**. Let us learn about them in detail in this activity.

> *Refer to src/Activity 4/Activity 4 - Detailed Data Analysis.ipynb*

## Summary

Now, you must be confident in the implementation of Linear Regression and its Regularized variations. Also, you must be clear in the algorithm and can debug any unusual behaviour that the model shows in any dataset.

## References

1. Dataset: [https://www.kaggle.com/shree1992/housedata](https://www.kaggle.com/shree1992/housedata)